# Adagrad()
Implementation of [AdaGrad optimization](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad)

# Linear_SVM()
Implementation of hinge loss function with the slack. This is a naive Implementation of SVM that doesn't include the bias (assumed to be Zero so the true distribution is assumed to be centered around Zero).
You can check out the true Implementation in the function [primalSVM]() within my [Kernels Implementation]()

# Regularized_Logistic_Regression()
Logistic Regression loss with l2 regularization.
